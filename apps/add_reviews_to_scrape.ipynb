{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>date</th>\n",
       "      <th>week_num</th>\n",
       "      <th>meta_score</th>\n",
       "      <th>user_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Streets</td>\n",
       "      <td>Original Pirate Material</td>\n",
       "      <td>October 22, 2002</td>\n",
       "      <td>43</td>\n",
       "      <td>90</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom Waits</td>\n",
       "      <td>Alice</td>\n",
       "      <td>May 7, 2002</td>\n",
       "      <td>19</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Queens of the Stone Age</td>\n",
       "      <td>Songs For The Deaf</td>\n",
       "      <td>August 27, 2002</td>\n",
       "      <td>35</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spoon</td>\n",
       "      <td>Kill The Moonlight</td>\n",
       "      <td>August 20, 2002</td>\n",
       "      <td>34</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wilco</td>\n",
       "      <td>Yankee Hotel Foxtrot</td>\n",
       "      <td>April 23, 2002</td>\n",
       "      <td>17</td>\n",
       "      <td>87</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9314</th>\n",
       "      <td>Bonny Doon</td>\n",
       "      <td>Longwave</td>\n",
       "      <td>March 23, 2018</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9315</th>\n",
       "      <td>A Perfect Circle</td>\n",
       "      <td>Eat the Elephant</td>\n",
       "      <td>April 20, 2018</td>\n",
       "      <td>16</td>\n",
       "      <td>68</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9316</th>\n",
       "      <td>Arthur Buck</td>\n",
       "      <td>Arthur Buck</td>\n",
       "      <td>June 15, 2018</td>\n",
       "      <td>24</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9317</th>\n",
       "      <td>Josh Rouse</td>\n",
       "      <td>Love in the Modern Age</td>\n",
       "      <td>April 13, 2018</td>\n",
       "      <td>15</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9318</th>\n",
       "      <td>Dean Wareham</td>\n",
       "      <td>Dean Wareham vs. Cheval Sombre</td>\n",
       "      <td>October 26, 2018</td>\n",
       "      <td>43</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9319 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       artist                           album  \\\n",
       "0                 The Streets        Original Pirate Material   \n",
       "1                   Tom Waits                           Alice   \n",
       "2     Queens of the Stone Age              Songs For The Deaf   \n",
       "3                       Spoon              Kill The Moonlight   \n",
       "4                       Wilco            Yankee Hotel Foxtrot   \n",
       "...                       ...                             ...   \n",
       "9314               Bonny Doon                        Longwave   \n",
       "9315         A Perfect Circle                Eat the Elephant   \n",
       "9316              Arthur Buck                     Arthur Buck   \n",
       "9317               Josh Rouse          Love in the Modern Age   \n",
       "9318             Dean Wareham  Dean Wareham vs. Cheval Sombre   \n",
       "\n",
       "                  date  week_num  meta_score  user_score  \n",
       "0     October 22, 2002        43          90          87  \n",
       "1          May 7, 2002        19          90          85  \n",
       "2      August 27, 2002        35          89          89  \n",
       "3      August 20, 2002        34          88          88  \n",
       "4       April 23, 2002        17          87          89  \n",
       "...                ...       ...         ...         ...  \n",
       "9314    March 23, 2018        12          68           0  \n",
       "9315    April 20, 2018        16          68          73  \n",
       "9316     June 15, 2018        24          68           0  \n",
       "9317    April 13, 2018        15          68           0  \n",
       "9318  October 26, 2018        43          68           0  \n",
       "\n",
       "[9319 rows x 6 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "###\n",
    "# a python file that takes a csv file that has scraped data from metacritics aggragting site\n",
    "# for albums and builds urls to scrape specific data for album images, number of critic and \n",
    "# user reviews, record labels and genre of the albums\n",
    "###\n",
    "def load_csv()\n",
    "# import csv file and make a dataframe\n",
    "    file_path = Path('..', 'data', 'historical_data',  'combined_csv.csv')\n",
    "    df = pd.read_csv(file_path)\n",
    "    return(df)\n",
    "\n",
    "\n",
    "# a very \n",
    "# scrape_df.loc[scrape_df.index[641], 'album'] = '#N/A'\n",
    "\n",
    "\n",
    "\n",
    "def create_review_url(df):\n",
    "###\n",
    "# takes a df latest metascrape, cleans the artist and album strings\n",
    "# and adds a new column of urls to scrape review values\n",
    "# some language to a handle any disconnects or timeouts if \n",
    "# running dataframes with more than 400 rows.\n",
    "###\n",
    "\n",
    "# a list for creating pickles if needed\n",
    "#     pickles = []\n",
    "\n",
    "\n",
    "    review_urls = []\n",
    "    # iterate over dataframe\n",
    "    for i, j in df.iterrows():\n",
    "    #     clean album and artist names of all nonalpha except for ! and accent marks\n",
    "        al = re.sub(r'[^-A-Za-z0-9!áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]+', '', j['album'])\n",
    "        ar = re.sub(r'[^-A-Za-z0-9!áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]+', '', j['artist'])\n",
    "    # correct instances of multilple spaces\n",
    "        al = re.sub(' +', ' ', al)\n",
    "        ar = re.sub(' +', ' ', ar)\n",
    "    # make lowercase and join words with - for url\n",
    "        url_end = f'{al}/{ar}'.replace(\" \", \"-\").lower() \n",
    "    # concat url for review scrape    \n",
    "        url_beginning ='https://www.metacritic.com/music/'\n",
    "        review_urls.append(url_beginning + url_end)\n",
    "        pickles.append(al+'-'+ar)\n",
    "# get value of colmuns to use for new column position\n",
    "    new_col_position = len(df.columns)\n",
    "# Use insert to add review_urls column and values to dataframe\n",
    "    df.insert(new_col_position, \"review_urls\", review_urls)\n",
    "    df.insert(new_col_position, 'pickle_name', pickles)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrape_reviews(df):\n",
    "    ###\n",
    "    # takes a meta_scrape dataframe with review urls added\n",
    "    # and scrapes the num of reviews-both critics and users-\n",
    "    # album image, record label and first listed genre of \n",
    "    # album and returns a dataframe\n",
    "    ###\n",
    "#     lists for collecting scraped data\n",
    "    img = []\n",
    "    crit_num = []\n",
    "    user_num = []\n",
    "    label = []\n",
    "    genre = []\n",
    "    count = 0\n",
    "###    \n",
    "# iterate over dataframe of scraped aggragate album rating from metacritic\n",
    "# for scraping review data for each album\n",
    "###\n",
    "    for i, j in df.iterrows():\n",
    "###\n",
    "# following section sets up for loop to create a pickle file for each album scrape\n",
    "# in case of dicsonnect or connection time out. checks for pickle files and picks\n",
    "# up from last created file.\n",
    "###\n",
    "#         count+=1\n",
    "# #         check for file in pickle if none then continue\n",
    "    \n",
    "#         if Path('../data/pickle/'+j['pickle_name']+'.pickle').is_file():\n",
    "#             # file exists\n",
    "#             print('Pity the fool who finds my file')\n",
    "#             with open('../data/pickle/'+j['pickle_name']+'.pickle', 'rb') as handle:\n",
    "#                 b = pickle.load(handle)\n",
    "#                 img.append(b['img'])\n",
    "#                 crit_num.append(b['crit_num'])\n",
    "#                 user_num.append(b['user_num'])\n",
    "#                 label.append(b['label'])\n",
    "#                 genre.append(b['genre'])\n",
    "                \n",
    "#             continue\n",
    "###\n",
    "# beginning of scrape of review page for each album\n",
    "###\n",
    "        url = j['review_urls']\n",
    "        user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "        # send response\n",
    "        response_reviews = requests.get(url, headers = user_agent)\n",
    "        # scrape website into variable to parse\n",
    "        soup_reviews = BeautifulSoup(response_reviews.text, 'html.parser')\n",
    "        ########### comment back in from here\n",
    "        # scrape album image \n",
    "        try:\n",
    "            img_soup = soup_reviews.find('meta', property=\"og:image\")\n",
    "            img.append(img_soup['content'])\n",
    "        except:\n",
    "            img.append(None)\n",
    "        # scrape num of critical reviews\n",
    "        try:\n",
    "            num_rev=(soup_reviews.find('span', itemprop=\"reviewCount\"))\n",
    "            crit_num.append(num_rev.text.strip())\n",
    "        except:\n",
    "            crit_num.append(None)\n",
    "        # scrape num of user reviews\n",
    "        if j['user_score'] == 0:\n",
    "            user_num.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                user_revs = float(soup_reviews.find('div', class_='metascore_w user large album positive').text)\n",
    "                user_num.append(int(user_revs*10))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                user_revs = float(soup_reviews.find('div', class_='metascore_w user large album mixed').text)\n",
    "                user_num.append(int(user_revs*10))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                user_revs = float(soup_reviews.find('div', class_='metascore_w user large album negative').text)\n",
    "                user_num.append(int(user_revs*10))\n",
    "                countneg+=1\n",
    "            except:\n",
    "                pass\n",
    "        # scrape record labels\n",
    "        try:\n",
    "            label_class = soup_reviews.find_all(\"span\", itemprop=\"name\")\n",
    "            label.append(label_class[2].text.strip())\n",
    "        except:\n",
    "            label.append(None)\n",
    "        # scrape genre\n",
    "        try:\n",
    "            genre.append(soup_reviews.find(\"span\", itemprop=\"genre\").text)\n",
    "        except:\n",
    "            genre.append(None)\n",
    "###\n",
    "# create pickle here for each album if needed with counter  \n",
    "###\n",
    "#         a = {\n",
    "#             'img': img[-1],\n",
    "#             'crit_num': crit_num[-1],\n",
    "#             'user_num': user_num[-1],\n",
    "#             'label': label[-1],\n",
    "#             'genre': genre[-1] \n",
    "#         }\n",
    "        \n",
    "#         with open('../data/pickle/'+j['pickle_name']+'.pickle', 'wb') as handle:\n",
    "#             pickle.dump(a, handle)\n",
    "#         print(j['artist'])\n",
    "#         print(f'{count} of 9319 run')\n",
    "      \n",
    "     \n",
    "       \n",
    "    # drop column for url and add scrapped lists to dataframe\n",
    "    df = df.drop(columns='review_urls').assign(album_img = img, crit_rev_num = crit_num, user_rev_num = user_num, record_label = label, album_genre = genre)\n",
    "    return df\n",
    "\n",
    "df = load_csv()\n",
    "full_scrape_df = scrape_reviews(create_review_url(df))\n",
    "\n",
    "full_scrape_df.to_csv('full_scrape_20yr' ,index=False)\n",
    "###\n",
    "# code snippet for removing pickle files after completed run\n",
    "###\n",
    "\n",
    "# pickle_path = Path('../data/pickle/')\n",
    "\n",
    "# pickle_files = pickle_path.glob('*.pickle')\n",
    "\n",
    "# for pickle in pickle_files:\n",
    "#     pickle.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('full_scrape_20yr' ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
