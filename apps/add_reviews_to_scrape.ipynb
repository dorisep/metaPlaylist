{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_date = datetime.date.today() \n",
    "# year, week_num, day_of_week = my_date.isocalendar()\n",
    "# this_week = week_num\n",
    "\n",
    "# url_for_scrape = 'https://www.metacritic.com/browse/albums/release-date/new-releases/date'\n",
    "# user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "# # send response\n",
    "# response_score = requests.get(url_for_scrape, headers = user_agent)\n",
    "# # scrape website into variable to parse\n",
    "# soup_score = BeautifulSoup(response_score.text, 'html.parser')\n",
    "\n",
    "# # create temporary lists for user scores\n",
    "# userP = []\n",
    "# userM = []\n",
    "# userN = []\n",
    "# # create/initialize dictionary \n",
    "# albums_dict = {'artist':[], 'album':[], 'date':[], 'week_num':[], 'meta_score': [], 'user_score':[]}\n",
    "\n",
    "# soup_score.find_all('td', class_='clamp-summary-wrap')\n",
    "# # create soup \n",
    "# for _ in soup_score.find_all('td', class_='clamp-summary-wrap'):\n",
    "#     # scrape album name\n",
    "#     albums_dict['album'].append(_.find('a', class_= 'title').text)\n",
    "#     # scrape artist name and strip white space and extra characters\n",
    "#     albums_dict['artist'].append(_.find('div', class_='artist').text.strip().lstrip('by '))\n",
    "#     # scrape date\n",
    "#     albums_dict['date'].append(_.find('div', class_='clamp-details').find('span').text)\n",
    "#     # scrape meta_score, handle for changes in class name, convert data type of score to int and append to dict\n",
    "#     # except set to pass since all alubms have a score\n",
    "\n",
    "#     try:\n",
    "#         albums_dict['meta_score'].append(int(_.find('div', class_='metascore_w large release positive').text))  \n",
    "#     except:\n",
    "#         pass\n",
    "#     try:\n",
    "#         albums_dict['meta_score'].append(int(_.find('div', class_='metascore_w large release mixed').text))  \n",
    "#     except:\n",
    "#         pass \n",
    "#     try:\n",
    "#         albums_dict['meta_score'].append(int(_.find('div', class_='metascore_w large release negative').text))  \n",
    "#     except:\n",
    "#         pass\n",
    "#     # scrape user score, handle errors for tbd/class name and append to temp list\n",
    "#     try:\n",
    "#         userP.append(float(_.find('div', class_='metascore_w user large release positive').text))  \n",
    "#     except:\n",
    "#         userP.append(0)\n",
    "#     try:\n",
    "#         userM.append(float(_.find('div', class_='metascore_w user large release mixed').text))  \n",
    "#     except:\n",
    "#         userM.append(0)\n",
    "#     try:\n",
    "#         userN.append(float(_.find('div', class_='metascore_w user large release negative').text))  \n",
    "#     except:\n",
    "#         userN.append(0)\n",
    "\n",
    "# # merge user score by filtering scores from tbd using data type in temporary lists, convert data type of scores to int and append to dictionary        \n",
    "# for a, b, c in zip(userP, userM, userN):\n",
    "#     if isinstance(a, float):\n",
    "#         albums_dict['user_score'].append(int(a * 10))\n",
    "#     elif isinstance(b, float):\n",
    "#         albums_dict['user_score'].append(int(b * 10))\n",
    "#     elif isinstance(c, float):\n",
    "#         albums_dict['user_score'].append(int(c * 10))\n",
    "#     else:\n",
    "#         albums_dict['user_score'].append(c)\n",
    "# # create week_num key and values for weekly scrape\n",
    "# for dates in albums_dict['date']:\n",
    "#     albums_dict['week_num'].append((datetime.datetime.strptime(dates, '%B %d, %Y')).isocalendar()[1])\n",
    "# # write dictionary to csv\n",
    "# # create header\n",
    "# fields = ['artist', 'album', 'date', 'week_num', 'meta_score', 'user_score'] \n",
    "# # create variable for data to be written\n",
    "# data = zip(albums_dict['artist'], albums_dict['album'], albums_dict['date'], albums_dict['week_num'], albums_dict['meta_score'], albums_dict['user_score'])\n",
    "# output_path = os.path.join('..', 'data', 'meta_scrape.csv')\n",
    "# # create header\n",
    "# fields = ['artist', 'album', 'date', 'week_num', 'meta_score', 'user_score'] \n",
    "# # create variable for data to be written\n",
    "# data = zip(\n",
    "#         albums_dict['artist'], \n",
    "#         albums_dict['album'], \n",
    "#         albums_dict['date'], \n",
    "#         albums_dict['week_num'], \n",
    "#         albums_dict['meta_score'], \n",
    "#         albums_dict['user_score']\n",
    "#         )\n",
    "# with open(output_path, 'a') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     for d in data:\n",
    "#         writer.writerow(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('..', 'data', 'meta_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>date</th>\n",
       "      <th>week_num</th>\n",
       "      <th>meta_score</th>\n",
       "      <th>user_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pokey LaFarge</td>\n",
       "      <td>In the Blossom of Their Shade</td>\n",
       "      <td>October 15, 2021</td>\n",
       "      <td>41</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hayden Thorpe</td>\n",
       "      <td>Moondust for My Diamond</td>\n",
       "      <td>October 15, 2021</td>\n",
       "      <td>41</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Music of the Spheres</td>\n",
       "      <td>October 15, 2021</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>64\\</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FINNEAS</td>\n",
       "      <td>Optimist</td>\n",
       "      <td>October 15, 2021</td>\n",
       "      <td>41</td>\n",
       "      <td>71</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remi Wolf</td>\n",
       "      <td>Juno</td>\n",
       "      <td>October 15, 2021</td>\n",
       "      <td>41</td>\n",
       "      <td>91</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>audiobooks</td>\n",
       "      <td>Astro Tough</td>\n",
       "      <td>October 1, 2021</td>\n",
       "      <td>39</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Ministry</td>\n",
       "      <td>Moral Hygiene</td>\n",
       "      <td>October 1, 2021</td>\n",
       "      <td>39</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Black Dice</td>\n",
       "      <td>Mod Prog Sic</td>\n",
       "      <td>October 1, 2021</td>\n",
       "      <td>39</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Boy Scouts</td>\n",
       "      <td>Wayfinder</td>\n",
       "      <td>October 1, 2021</td>\n",
       "      <td>39</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Andy Shauf</td>\n",
       "      <td>Wilds</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>38</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            artist                          album                date  \\\n",
       "0    Pokey LaFarge  In the Blossom of Their Shade    October 15, 2021   \n",
       "1    Hayden Thorpe        Moondust for My Diamond    October 15, 2021   \n",
       "2         Coldplay           Music of the Spheres    October 15, 2021   \n",
       "3          FINNEAS                       Optimist    October 15, 2021   \n",
       "4        Remi Wolf                           Juno    October 15, 2021   \n",
       "..             ...                            ...                 ...   \n",
       "155     audiobooks                    Astro Tough     October 1, 2021   \n",
       "156       Ministry                  Moral Hygiene     October 1, 2021   \n",
       "157     Black Dice                   Mod Prog Sic     October 1, 2021   \n",
       "158     Boy Scouts                      Wayfinder     October 1, 2021   \n",
       "159     Andy Shauf                          Wilds  September 24, 2021   \n",
       "\n",
       "     week_num  meta_score user_score  \n",
       "0          41          85          0  \n",
       "1          41          77          0  \n",
       "2          41          57        64\\  \n",
       "3          41          71         89  \n",
       "4          41          91         88  \n",
       "..        ...         ...        ...  \n",
       "155        39          73          0  \n",
       "156        39          71          0  \n",
       "157        39          78          0  \n",
       "158        39          74          0  \n",
       "159        38          82          0  \n",
       "\n",
       "[160 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_df = pd.read_csv(file_path)  \n",
    "# dedup df        \n",
    "scrape_clean = scrape_df.drop_duplicates(subset = 'artist', ignore_index = True)\n",
    "scrape_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reviews =[]\n",
    "def create_review_url(df):\n",
    "###\n",
    "# takes a df latest metascrape, cleans the artist and album values\n",
    "# and adds a new column of urls to scrape review values\n",
    "###\n",
    "    review_urls = []\n",
    "    # iterate over dataframe\n",
    "    for i, j in scrape_clean.iterrows():\n",
    "    #     clean album and artist names of all nonalpha except for ! and accent marks\n",
    "        al = re.sub(r'[^-A-Za-z0-9!áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]+', '', j['album'])\n",
    "        ar = re.sub(r'[^-A-Za-z0-9!áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]+', '', j['artist'])\n",
    "    # correct instances of multilple spaces\n",
    "        al = re.sub(' +', ' ', al)\n",
    "        ar = re.sub(' +', ' ', ar)\n",
    "    # make lowercase and join words with - for url\n",
    "        url_end = f'{al}/{ar}'.replace(\" \", \"-\").lower() \n",
    "    # concat url for review scrape    \n",
    "        url_beginning ='https://www.metacritic.com/music/'\n",
    "        review_urls.append(url_beginning + url_end)\n",
    "# get value of colmuns to use for new column position\n",
    "    new_col_position = len(df.columns)\n",
    "# Use insert to add review_urls column and values to dataframe\n",
    "    df.insert(new_col_position, \"review_urls\", review_urls)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_clean_url_df = create_review_url(scrape_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# found an errant / in user reviews and column is of datatype string and don't know why\n",
    "# this fixes that but need to check scrape function to see why\n",
    "###\n",
    "# for _ in scrape_clean_url_df['user_score']:\n",
    "#     if len(_) > 2:\n",
    "#         _=_[:2]\n",
    "        \n",
    "# scrape_clean_url_df = int(scrape_clean_url_df['user_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"label\">Record Label:</span>\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# artist= {}\n",
    "# def scrape_review_page(df):\n",
    "# df = df.assign(Admissionnum=[250, 800, 1200, 300, 400, 700], \n",
    "#                Percentage=['85%', '90%', '75%', '35%', '60%', '80%'])\n",
    "img = []\n",
    "crit_num = []\n",
    "user_num = []\n",
    "\n",
    "for i, j in scrape_clean_url_df.iterrows():\n",
    "    url = j['review_urls']\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    # send response\n",
    "    response_reviews = requests.get(url, headers = user_agent)\n",
    "    # scrape website into variable to parse\n",
    "    soup_reviews = BeautifulSoup(response_reviews.text, 'html.parser')\n",
    "    ########### comment back in from here\n",
    "#     # scrape album image \n",
    "#     img_soup = soup_reviews.find('meta', property=\"og:image\")\n",
    "#     img.append(img_soup['content'])\n",
    "#     # scrape num of critical reviews\n",
    "#     num_rev=(soup_reviews.find('span', itemprop=\"reviewCount\"))\n",
    "#     crit_num.append(num_rev.text.strip())\n",
    "# #     user_review_href = f'{url[27:]}/user-reviews'\n",
    "# #     print(user_review_href)\n",
    "#     # num of user reviews\n",
    "#     if j['user_score'] == '0':\n",
    "#         user_revs = 0\n",
    "#         user_num.append(user_revs)\n",
    "#     else:\n",
    "#         try:\n",
    "#             user_revs = float(soup_reviews.find('div', class_='metascore_w user large album positive').text)\n",
    "#             user_num.append(int(user_revs*10))\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             user_revs = float(soup_reviews.find('div', class_='metascore_w user large album mixed').text)\n",
    "#             user_num.append(int(user_revs*10))\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             user_revs = float(soup_reviews.find('div', class_='metascore_w user large album negative').text)\n",
    "#             user_num.append(int(user_revs*10))\n",
    "#         except:\n",
    "#             pass\n",
    "############## to here\n",
    "    label_class = soup_reviews.find('span', class_='label')\n",
    "#     label_data = label_class.selector(class_= 'label')\n",
    "    print(label_class.)\n",
    "    print('--------------')\n",
    "#     print(label_data)\n",
    "    break\n",
    "   \n",
    "# scrape_complete_df = scrape_clean_url_df.assign(album_img = img, crit_rev_num = crit_num, user_rev_num = user_num)\n",
    "# scrape_complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'div', class_='metascore_w user large album mixed'\n",
    "'div', class_='metascore_w user large release negative'\n",
    "#     artist['user_rev_num'] = \n",
    "#     artist['critic_rev_num']\n",
    "    break\n",
    "#     for link in soup_reviews.find_all('div', class_='product_image large_image'):\n",
    "#         print(link.meta.descendents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /html/body/div[1]/div[2]/div[1]/div[1]/div/div/div/div/div/div/div/div/div[1]/div[1]/div[3]/div/div[2]/div[2]/div[2]/ul/li[1]/span[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <img alt=\"The Myth of the Happily Ever After Image\" class=\"product_image large_image\" src=\"https://static.metacritic.com/images/products/music/8/3f66ba56e8190a1a9f04fb37bdd986e8-98.jpg\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a Connection with MongoClient\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "# database\n",
    "db = client[\"stocks_database\"]\n",
    "# collection\n",
    "company= db[\"Company\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
